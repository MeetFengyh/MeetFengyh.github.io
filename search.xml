<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>kerberos部署</title>
      <link href="/2022/051763987.html"/>
      <url>/2022/051763987.html</url>
      
        <content type="html"><![CDATA[<h1 id="kerberos部署"><a href="#kerberos部署" class="headerlink" title="kerberos部署"></a>kerberos部署</h1><h2 id="安装kerberos相关服务"><a href="#安装kerberos相关服务" class="headerlink" title="安装kerberos相关服务"></a>安装kerberos相关服务</h2><h3 id="kerberos服务端部署"><a href="#kerberos服务端部署" class="headerlink" title="kerberos服务端部署"></a>kerberos服务端部署</h3><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# yum install -y krb5-server</code></pre><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509162205790.png" alt="image-20220509162205790"></p><h3 id="kerberos客户端部署"><a href="#kerberos客户端部署" class="headerlink" title="kerberos客户端部署"></a>kerberos客户端部署</h3><p>在所有需要进行认证的服务器安装客户端（服务端服务器同样要安装）</p><pre><code>[root@hadoop01 ~]# yum install -y krb5-workstation krb5-libs[root@hadoop02 ~]# yum install -y krb5-workstation krb5-libs[root@hadoop03 ~]# yum install -y krb5-workstation krb5-libs</code></pre><p>服务端机器安装客户端截图</p><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509162242443.png" alt="image-20220509162242443"></p><p>其余服务器安装客户端截图</p><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509162258103.png" alt="image-20220509162258103"></p><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><h3 id="修改kdc-conf"><a href="#修改kdc-conf" class="headerlink" title="修改kdc.conf"></a>修改kdc.conf</h3><pre><code>[root@hadoop01 ~]# vim /var/kerberos/krb5kdc/kdc.conf</code></pre><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509162503285.png" alt="image-20220509162503285"></p><p>将 EXAMPLE.COM修改成公司域名，非必需，不改也可以</p><h3 id="修改客户端配置文件"><a href="#修改客户端配置文件" class="headerlink" title="修改客户端配置文件"></a>修改客户端配置文件</h3><h4 id="修改-x2F-etc-x2F-krb5-conf文件"><a href="#修改-x2F-etc-x2F-krb5-conf文件" class="headerlink" title="修改&#x2F;etc&#x2F;krb5.conf文件"></a>修改&#x2F;etc&#x2F;krb5.conf文件</h4><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# vim /etc/krb5.conf[root@hadoop02 ~]# vim /etc/krb5.conf[root@hadoop03 ~]# vim /etc/krb5.conf</code></pre><p>添加参数dns_lookup_kdc &#x3D; false</p><p>添加默认realm</p><p>注释掉参数default_ccache_name</p><p>取消[realms]的注释</p><pre class=" language-Linux"><code class="language-Linux">includedir /etc/krb5.conf.d/[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false pkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crt default_realm = HCYY.COM # default_ccache_name = KEYRING:persistent:%&#123;uid&#125;[realms] EXAMPLE.COM = &#123;  # 服务端主机名  kdc = hadoop01  # 服务端主机名  admin_server = hadoop01 &#125;</code></pre><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509163511742.png" alt="image-20220509163511742"></p><h2 id="初始化KDC数据库"><a href="#初始化KDC数据库" class="headerlink" title="初始化KDC数据库"></a>初始化KDC数据库</h2><p>在服务端主机执行以下命令，并根据提示输入密码，创建管理员用户密码</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# kdb5_util create -s</code></pre><h3 id="修改管理员权限配置文件"><a href="#修改管理员权限配置文件" class="headerlink" title="修改管理员权限配置文件"></a>修改管理员权限配置文件</h3><p>在服务端主机，修改&#x2F;var&#x2F;kerberos&#x2F;krb5kdc&#x2F;kadm5.acl文件，内容如下，如果没有修改realm，下面文件不需要修改，如果有修改realm，需要对应修改EXAMPLE.COM</p><pre class=" language-Linux"><code class="language-Linux">*/admin@EXAMPLE.COM     *</code></pre><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509164944802.png" alt="image-20220509164944802"></p><h2 id="启动Kerberos相关服务"><a href="#启动Kerberos相关服务" class="headerlink" title="启动Kerberos相关服务"></a>启动Kerberos相关服务</h2><p>在主节点启动KDC，并配置开机自启</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# systemctl start krb5kdc[root@hadoop01 ~]# systemctl enable krb5kdc</code></pre><p>在主节点启动Kadmin，该服务是KDC数据库访问入口</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# systemctl start kadmin[root@hadoop01 ~]# systemctl enable kadmin</code></pre><h2 id="创建Kerberos管理员用户"><a href="#创建Kerberos管理员用户" class="headerlink" title="创建Kerberos管理员用户"></a>创建Kerberos管理员用户</h2><p>在KDC主机，执行以下命令，并按提示输入密码</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# kadmin.local -q "addprinc admin/admin"</code></pre><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509170140264.png" alt="image-20220509170140264"></p><h1 id="Kerberos使用概述"><a href="#Kerberos使用概述" class="headerlink" title="Kerberos使用概述"></a>Kerberos使用概述</h1><h2 id="Kerberos数据库操作"><a href="#Kerberos数据库操作" class="headerlink" title="Kerberos数据库操作"></a>Kerberos数据库操作</h2><h3 id="登录数据库"><a href="#登录数据库" class="headerlink" title="登录数据库"></a>登录数据库</h3><h4 id="本地登录"><a href="#本地登录" class="headerlink" title="本地登录"></a>本地登录</h4><pre><code>[root@hadoop01 ~]# kadmin.local</code></pre><p>点击tab查看</p><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509170356062.png" alt="image-20220509170356062"></p><h4 id="远程登录"><a href="#远程登录" class="headerlink" title="远程登录"></a>远程登录</h4><p>此时直接远程登录会报错，当前KDC数据库root用户的信息，没有创建root的用户，且root属于admin组才可以进入</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop02 ~]# kadmin</code></pre><p><img src="C:\Users\hc0500036\AppData\Roaming\Typora\typora-user-images\image-20220509170634122.png" alt="image-20220509170634122"></p><h3 id="创建Kerberos主体"><a href="#创建Kerberos主体" class="headerlink" title="创建Kerberos主体"></a>创建Kerberos主体</h3><pre class=" language-Linux"><code class="language-Linux"># 创建kadmin.local:  addprinc test# 查看kadmin.local:  list_principals</code></pre><h3 id="修改主体密码"><a href="#修改主体密码" class="headerlink" title="修改主体密码"></a>修改主体密码</h3><pre class=" language-Linux"><code class="language-Linux">kadmin.local:  cpw test</code></pre><h3 id="删除Kerberos主体"><a href="#删除Kerberos主体" class="headerlink" title="删除Kerberos主体"></a>删除Kerberos主体</h3><pre class=" language-Linux"><code class="language-Linux">kadmin.local:  delprinc test</code></pre><h2 id="Kerberos认证操作"><a href="#Kerberos认证操作" class="headerlink" title="Kerberos认证操作"></a>Kerberos认证操作</h2><h3 id="密码认证"><a href="#密码认证" class="headerlink" title="密码认证"></a>密码认证</h3><p>使用kinit进行主体认证，并按照提示输入密码，没有报错就是认证成功</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# kinit test</code></pre><p>查看认证凭证</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# kinit testTicket cache: FILE:/tmp/krb5cc_0Default principal: test@HCYY.COMValid starting       Expires              Service principal2022-05-09T17:14:57  2022-05-10T17:14:57  krbtgt/HCYY.COM@HCYY.COM</code></pre><h3 id="使用密钥文件认证"><a href="#使用密钥文件认证" class="headerlink" title="使用密钥文件认证"></a>使用密钥文件认证</h3><p>生成主体test的keytab文件到指定目录&#x2F;root&#x2F;test.keytab</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# kadmin.local -q "xst -norandkey -k /root/test.keytab test@HCYY.COM"</code></pre><p>使用keytab进行认证</p><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# kinit -kt /root/test.keytab test</code></pre><h3 id="销毁凭证"><a href="#销毁凭证" class="headerlink" title="销毁凭证"></a>销毁凭证</h3><pre class=" language-Linux"><code class="language-Linux">[root@hadoop01 ~]# kdestroy[root@hadoop01 ~]# klist</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hive调优实战.md</title>
      <link href="/2022/041816948.html"/>
      <url>/2022/041816948.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hive基础参数"><a href="#Hive基础参数" class="headerlink" title="Hive基础参数"></a>Hive基础参数</h1><p>参考链接：<a href="https://cloud.tencent.com/developer/article/1812346">Hive参数调优 - 云+社区 - 腾讯云 (tencent.com)</a></p><h1 id="Hive调优"><a href="#Hive调优" class="headerlink" title="Hive调优"></a>Hive调优</h1><h2 id="添加JVM重用"><a href="#添加JVM重用" class="headerlink" title="添加JVM重用"></a>添加JVM重用</h2><p>一个job内，多个task共享jvm，避免多次启动jvm，浪费资源和时间。</p><p>优点：减少频繁起停jvm的开销。</p><p>缺点：任务全部完成，设置重用的task才会全部释放，如果单个任务执行较长，会导致资源占用。</p><p>适用场景：任务数较多或者任务执行速度较快，会频繁起停jvm的任务。</p><pre><code>set mapred.job.reuse.jvm.num.tasks=10;</code></pre><h2 id="文件压缩"><a href="#文件压缩" class="headerlink" title="文件压缩"></a>文件压缩</h2><p>是否选择文件压缩：<br>在hadoop作业执行过程中，job执行速度更多的是局限于I&#x2F;O，而不是受制于CPU。如果是这样，通过文件压缩可以提高hadoop性能。然而，如果作业的执行速度受限于CPU的性能，呢么压缩文件可能就不合适，因为文件的压缩和解压会花费掉较多的时间。当然确定适合集群最优配置的最好方式是通过实验测试，然后衡量结果。</p><h3 id="中间压缩"><a href="#中间压缩" class="headerlink" title="中间压缩"></a>中间压缩</h3><p>中间压缩就是处理作业map任务和reduce任务之间的数据，对于中间压缩，最好选择一个节省CPU耗时的压缩方式。</p><pre><code>set hive.exec.compress.intermediate = true;set mapred.output.compression.codec = org.apache.hadoop.io.compress.SnappyCodec;</code></pre><h3 id="最终的压缩输出"><a href="#最终的压缩输出" class="headerlink" title="最终的压缩输出"></a>最终的压缩输出</h3><p>作业最终的输出可以压缩。</p><pre><code>set hive.exec.compress.output = true;set mapred.output.compression.codec = org.apache.hadoop.io.compress.SnappyCodec;</code></pre><h2 id="设置切片大小"><a href="#设置切片大小" class="headerlink" title="设置切片大小"></a>设置切片大小</h2><p>mapreduce根据切片启动map任务，一个切片启动一个map读取。</p><pre><code> // 计算切片大小 computeSplitSize(Math.max(&quot;mapred.min.split.size&quot;,min(&quot;mapred.max.split.size&quot;,blockSize)))=blockSize=128M(默认)</code></pre><p>切片原理：</p><ul><li>简单的按照文件内容长度进行切片。</li><li>切片大小，默认等于128MB，即blocksize的大小(我们这边设置为256M)。</li><li>切片时不考虑数据整体，而是针对每一个文件单独切片。</li></ul><p>适用场景：</p><ul><li>小文件数量过多，设置虚拟切片大小，用于合并小文件，减少map数量。</li><li>在文件支持切割的情况下(有的压缩格式不支持切割，如gzip)，设置mapred.max.split.size小于blocksize可以增多map数量，减少每个map处理的数据量，不容易出现oom。</li></ul><pre><code>// hive设置如下，单位byteset mapred.max.split.size=50000000;set mapred.min.split.size.per.rack=1;set mapred.min.split.size.per.node=1;</code></pre><h2 id="开启本地Mr模式"><a href="#开启本地Mr模式" class="headerlink" title="开启本地Mr模式"></a>开启本地Mr模式</h2><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p><pre><code> (1)开启本地mr set hive.exec.mode.local.auto=true; (2)设置local mr的最大输入数据量,当输入数据量小于这个值的时候会采用local mr的方式 set hive.exec.mode.local.auto.inputbytes.max=50000000; (3)设置local mr的最大输入文件个数,当输入文件个数小于这个值的时候会采用local mr的方式 set hive.exec.mode.local.auto.tasks.max=10; 当这三个参数同时成立时候，才会采用本地mr。</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>yarn资源调度器.md</title>
      <link href="/2022/041835875.html"/>
      <url>/2022/041835875.html</url>
      
        <content type="html"><![CDATA[<h1 id="Yarn基础架构"><a href="#Yarn基础架构" class="headerlink" title="Yarn基础架构"></a>Yarn基础架构</h1><p>yarn主要由ResourceManager，NodeManager，ApplicationMaster，Container等组件构成。</p><p>ResourceManager 作为主守护程序运行，该仲裁程序在各种竞争应用程序之间仲裁可用的群集资源。ResourceManager 跟踪群集上可用的活动节点和资源的数量，并协调用户提交的应用程序应获取这些资源的时间和时间。ResourceManager 是具有此信息的单个进程，因此它可以以共享，安全和多租户的方式进行分配（或者更确切地说，调度）决策（例如，根据应用程序优先级，队列容量，ACL，数据位置等）。</p><p>当用户提交应用程序时，将启动名为 ApplicationMaster 的轻量级进程实例，以协调应用程序中所有任务的执行。这包括监视任务，重新启动失败的任务，推测性地运行慢速任务以及计算应用程序计数器的总值。这些职责先前已分配给所有工作的单个 JobTracker。ApplicationMaster 和属于其应用程序的任务在 NodeManagers 控制的资源容器中运行。</p><p>ApplicationMaster 可以在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster 请求容器启动 map 或 reduce 任务，而 Giraph ApplicationMaster 请求容器运行 Giraph 任务。您还可以实现运行特定任务的自定义 ApplicationMaster，并以此方式创建一个闪亮的新分布式应用程序框架，该框架可以更改大数据世界。我鼓励您阅读 Apache Twill，它旨在简化编写位于 YARN 之上的分布式应用程序。</p><p>一个可以运行任何分布式应用程序的集群 ResourceManager，NodeManager 和容器不关心应用程序或任务的类型。所有特定于应用程序框架的代码都被简单地移动到其 ApplicationMaster，以便 YARN 可以支持任何分布式框架，只要有人为它实现适当的 ApplicationMaster。由于这种通用方法，运行许多不同工作负载的 Hadoop YARN 集群的梦想成真。想象一下：数据中心内的单个 Hadoop 集群可以运行 MapReduce，Giraph，Storm，Spark，Tez &#x2F; Impala，MPI 等。</p><h2 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h2><p>RM 是一个全局的资源管理器，集群只有一个，负责整个系统的资源管理和分配，包括处理客户端请求、启动&#x2F;监控 ApplicationMaster、监控 NodeManager、资源的分配与调度。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，ASM）。</p><h2 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h2><p>调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的 ApplicationMaster 完成。</p><p>调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（ResourceContainer，简称 Container）表示，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。</p><h2 id="应用程序管理器"><a href="#应用程序管理器" class="headerlink" title="应用程序管理器"></a>应用程序管理器</h2><p>应用程序管理器主要负责管理整个系统中所有应用程序，接收 job 的提交请求，为应用分配第一个 Container 来运行 ApplicationMaster，包括应用程序提交、与调度器协商资源以启动 ApplicationMaster、监控ApplicationMaster 运行状态并在失败时重新启动它等。</p><h2 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h2><p>管理 YARN 内运行的一个应用程序的每个实例。关于 job 或应用的管理都是由 ApplicationMaster 进程负责的，Yarn 允许我们以为自己的应用开发 ApplicationMaster。</p><p>功能：(1)数据切分；(2)为应用程序申请资源并进一步分配给内部任务（TASK）；(3)任务监控与容错。</p><p>负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容易的执行和资源使用情况。Yarn 的动态性，就是来源于多个 Application 的 ApplicationMaster 动态地和 ResourceManager 进行沟通，不断地申请、释放、再申请、再释放资源的过程。</p><h2 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h2><p>NodeManager 整个集群有多个，负责每个节点上的资源和使用。NodeManager 是一个 slave 服务：它负责接收 ResourceManager 的资源分配请求，分配具体的 Container 给应用。同时，它还负责监控并报告 Container 使用信息给 ResourceManager。通过和 ResourceManager 配合，NodeManager 负责整个 Hadoop 集群中的资源分配工作。</p><p>功能：本节点上的资源使用情况和各个 Container 的运行状态（cpu 和内存等资源）</p><p>(1)接收及处理来自 ResourceManager 的命令请求，分配 Container 给应用的某个任务；(2)定时地向 RM 汇报以确保整个集群平稳运行，RM 通过收集每个 NodeManager 的报告信息来追踪整个集群健康状态的，而 NodeManager 负责监控自身的健康状态；(3)处理来自 ApplicationMaster 的请求；(4)管理着所在节点每个 Container 的生命周期；(5)管理每个节点上的日志；(6)执行 Yarn 上面应用的一些额外的服务，比如 MapReduce 的 shuffle 过程。</p><p>当一个节点启动时，它会向 ResourceManager 进行注册并告知 ResourceManager 自己有多少资源可用。在运行期，通过 NodeManager 和 ResourceManager 协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态。NodeManager 只负责管理自身的 Container，它并不知道运行在它上面应用的信息。负责管理应用信息的组件是 ApplicationMaster。</p><h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><p>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。YARN 会为每个任务分配一个 Container，且任务只能使用该 Container 中描述的资源。</p><p>Container 和集群节点的关系是：一个节点会运行多个 Container，但一个 Container 不会跨节点。任何一个 job 或 application 必须运行在一个或多个 Container 中，在 Yarn 框架中，ResourceManager 只负责告诉 ApplicationMaster 哪些 Containers 可以用，ApplicationMaster 还需要去找 NodeManager 请求分配具体的 Container。</p><p>需要注意的是，Container 是一个动态资源划分单位，是根据应用程序的需求动态生成的。目前为止，YARN 仅支持 CPU 和内存两种资源，且使用了轻量级资源隔离机制 Cgroups 进行资源隔离。</p><p>Resource Request 及 Container<br>Yarn 的设计目标就是允许我们的各种应用以共享、安全、多租户的形式使用整个集群。并且，为了保证集群资源调度和数据访问的高效性，Yarn 还必须能够感知整个集群拓扑结构。</p><p>为了实现这些目标，ResourceManager 的调度器 Scheduler 为应用程序的资源请求定义了一些灵活的协议，通过它就可以对运行在集群中的各个应用做更好的调度，因此，这就诞生了 Resource Request 和 Container。一个应用先向 ApplicationMaster 发送一个满足自己需求的资源请求，然后 ApplicationMaster 把这个资源请求以resource-request 的形式发送给 ResourceManager 的 Scheduler，Scheduler 再在这个原始的 resource-request 中返回分配到的资源描述 Container。</p><p><img src="D:\hc_data\document\hcyy-dw-resource\fyh\image\yarn基础架构.jpg"></p><p>ResourceManager:</p><ol><li>处理客户端请求</li><li>监控NodeManager</li><li>启动或监控ApplicationMaster</li><li>资源分配与调度</li></ol><p>NodeManager:</p><ol><li>管理单个节点的资源</li><li>处理来自ResourceManager的命令</li><li>处理来自ApplicationMaster的命令</li></ol><p>ApplicationMaster:</p><ol><li>为应用程序申请资源并分配内部任务</li><li>Map、Reduce任务的监控与容错</li></ol><p>Container:</p><ul><li>是yarn资源的抽象，封装了多维度的资源，如内存、cpu、网络、磁盘等。</li></ul><h1 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h1><p><img src="D:\hc_data\document\hcyy-dw-resource\fyh\image\yarn工作机制.jpg" alt="yarn工作机制"></p><ol><li>MR程序提交到客户端所在节点，向ResourceManager申请执行一个应用。</li><li>ResourceManager经过判断，有资源可以分配，批准可以执行应用，返回Applicationid和资源文件上传临时路径。</li><li>在准备阶段，job会生成3个文件放在第二步，分别是job.split，job.xml，jar包。job.split保存的是任务的切片信息，ApplicationMaster会根据切片信息申请执行的maptask。job.xml存放的是yarn和hdfs的配置信息。</li><li>开始申请运行mrAppMaster,这个进程负责你整个Job,整个运行期间状态监控,容错,资源申请等等.一个进程在Yarn上运行必须要申请Container.Container里面有cpu内存资源和mrAppmaster提供你Job的运行。</li><li>向ResourceManager提交请求,请求会初始化一个Task, 然后进入ResourceManager的调度队列里面。</li><li>NodeManager负责从ResourceManager的队列里面来领取任务,领取之后给你封装出一个Container</li><li>创建完Container之后里面有MRAppMaster运行需要的CPU和内存资源等等。</li><li>然后MRAppMaster一运行就会把之前步骤2目录那里生成的文件下载到本地,紧接着会读取本件里面的内容,读到job.xml里面的数据就知道要启动几个MapTask和几个ReduceTask了。</li><li>开始向ResourceManager继续申请运行MapTask容器,运行几个得看job.xml里面的值.同样申请也是会进到步骤五那里的ResourceManager调度队列里面。</li><li>NodeManager从ResourceManager调度队列里面领取到任务,就开始创建容器运行MapTask。</li><li>MrAppMaster会把步骤2的wc.jar和job.xml配置文件和MrAppMaster自动生成的程序的启动脚本发给步骤10的MapTask,然后就运行脚本,就启动程序了,紧接着MapTask就生成自己的结果了。</li><li>然后向步骤五的ResourceManager申请任务,进入调度队列里面,申请成功之后创建Container容器后,会运行ReduceTask任务。</li><li>ReduceTask向Map获取相应的分区数据</li><li>程序运行完成之后,MRAppMaster会向ResourceManager申请注销掉自己.注销之后Container容器就会被销毁,资源就被释放。</li></ol>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
